{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7e6a96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e0f94d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleaned_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77322e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "\n",
    "# Define a function to train the XGBoost model with hyperparameter tuning\n",
    "def train_xgboost_with_metrics(X_train, y_train, X_test, y_test):\n",
    "    # Initialize the XGBoost regressor\n",
    "    xgb_model = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "    # Define a hyperparameter grid for tuning\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "    }\n",
    "\n",
    "    # Set up GridSearchCV with 5-fold cross-validation\n",
    "    grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best estimator\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred_xgb = best_model.predict(X_test)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
    "    mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "    mape_xgb = np.mean(np.abs((y_test - y_pred_xgb) / y_test)) * 100\n",
    "\n",
    "    # Print best parameters and evaluation metrics\n",
    "    print(\"Best Parameters:\", grid_search.best_params_)\n",
    "    print(\"RMSE - XGBoost:\", rmse_xgb)\n",
    "    print(\"MAE - XGBoost:\", mae_xgb)\n",
    "    print(\"MAPE - XGBoost:\", mape_xgb)\n",
    "    \n",
    "    # Hyperparameter Sensitivity Analysis\n",
    "    results = grid_search.cv_results_\n",
    "    for param in param_grid.keys():\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(results[f'param_{param}'], results['mean_test_score'])\n",
    "        plt.xlabel(param)\n",
    "        plt.ylabel('Mean test score')\n",
    "        plt.title(f'Hyperparameter Sensitivity: {param}')\n",
    "        plt.show()\n",
    "\n",
    "    # Feature Importance\n",
    "    feature_importance = best_model.feature_importances_\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "    plt.yticks(pos, X_train.columns[sorted_idx])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title('XGBoost Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # SHAP Values for Feature Importance\n",
    "    explainer = shap.TreeExplainer(best_model)\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(shap_values, X_test, plot_type=\"bar\", show=False)\n",
    "    plt.title('SHAP Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Return the best model and metrics\n",
    "    return best_model, rmse_xgb, mae_xgb, mape_xgb\n",
    "\n",
    "# Example usage\n",
    "# Standardize the features as before\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# # Train the XGBoost model with hyperparameter tuning and get metrics\n",
    "# best_xgb_model, rmse_xgb, mae_xgb, mape_xgb = train_xgboost_with_metrics(X_train_scaled, y_train, X_test_scaled, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed346f0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6840bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33efe2e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105f219d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee97c2c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
