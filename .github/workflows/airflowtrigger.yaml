name: Trigger Airflow DAG Workflow

on:
  push:
    branches:
      - vy
  workflow_dispatch:

jobs:
  trigger_airflow_dag:
    runs-on: ubuntu-latest

    steps:
      # Step 1: Checkout the repository
      - name: Checkout repository
        uses: actions/checkout@v2

      # Step 2: Set up Python environment 
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.10'

      # Step 3: Install Dependencies 
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # Step 3.5: Install Docker Compose
      - name: Install Docker Compose
        run: |
          curl -SL https://github.com/docker/compose/releases/download/v2.30.3/docker-compose-linux-x86_64 -o /usr/local/bin/docker-compose
          # sudo apt-get update
          # sudo apt-get install -y docker-compose
          sudo chmod +x /usr/local/bin/docker-compose

      ## Step 3.1. Init db
      - name: Init Airflow DB
        working-directory: ./pipeline/airflow
        run: |
          docker-compose up airflow-init 

      # Step 4: Start Airflow using Docker Compose
      - name: Start Airflow Services
        working-directory: ./pipeline/airflow
        run: |
          mkdir -p ./dags ./logs ./plugins ./working_data
          docker-compose up -d
   
      - name: make env
        working-directory: ./pipeline/airflow
        run: |
          echo -e "AIRFLOW_UID=$(id -u)" > .env
      
      # Step 5: Set permissions for Airflow logs inside the container
      - name: Set permissions for Airflow logs
        working-directory: ./pipeline/airflow
        run: |
          docker-compose exec -T --user root airflow-scheduler bash -c "chmod -R 777 /opt/airflow/logs/"

      # Step 6: Wait for Airflow Services to be Up
      - name: Wait for Airflow to initialize
        working-directory: ./pipeline/airflow
        run: |
          sleep 400  # Wait a few minutes for Airflow to fully initialize

      # # Step 6.5: Initialize Airflow Database
      # - name: Initialize Airflow Database
      #   working-directory: ./pipeline/airflow
      #   run: |
      #     docker-compose exec -T airflow-scheduler airflow db init

      # Step 8: Trigger DAG via Airflow CLI
      - name: Trigger Airflow DAG
        working-directory: ./pipeline/airflow
        run: |
          docker-compose exec -T airflow-scheduler airflow dags trigger -r manual_$(date +%Y%m%d%H%M%S) Group10_DataPipeline_MLOps
      
      # Step 9: Monitor Airflow DAG Execution 
      - name: Monitor DAG Execution
        working-directory: ./pipeline/airflow
        run: |
          while true; do
            STATUS=$(docker compose exec -T airflow-scheduler airflow dags state Group10_DataPipeline_MLOps $(date +%Y-%m-%d))
            echo "Current DAG status: $STATUS"
            if [ "$STATUS" = "success" ]; then
              echo "DAG completed successfully"
              break
            elif [ "$STATUS" = "failed" ]; then
              echo "DAG failed"
              exit 1
            fi
            sleep 60  # Wait before checking the status again
          done

      # Step 10: Stop Airflow Services
      - name: Stop Airflow Services
        if: always()
        working-directory: ./pipeline/airflow
        run: docker compose down --volumes --rmi all