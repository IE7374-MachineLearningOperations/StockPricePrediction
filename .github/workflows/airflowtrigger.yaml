name: Trigger Airflow DAG Workflow

on:
  push:
    branches:
      - vy
  workflow_dispatch:

jobs:
  trigger_airflow_dag:
    runs-on: ubuntu-latest

    steps:
      # Step 1: Checkout the repository
      - name: Checkout repository
        uses: actions/checkout@v2

      # Step 2: Set up Python environment 
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.10'

      # Step 3: Install Dependencies 
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # Step 3.5: Install Docker Compose
      - name: Install Docker Compose
        run: |
          sudo apt-get update
          sudo apt-get install -y docker-compose

      # Step 4: Start Airflow using Docker Compose
      - name: Start Airflow Services
        working-directory: ./pipeline/airflow
        run: |
          mkdir -p ./dags ./logs ./plugins
          docker-compose up -d
      
      # Step 5: Set permissions for Airflow logs inside the container
      - name: Set permissions for Airflow logs
        working-directory: ./pipeline/airflow
        run: |
          docker-compose exec -T --user root airflow-scheduler bash -c "chmod -R 777 /opt/airflow/logs/"

      ## step 5.1. install the required packages inside docker image
      - name: Install required packages in Airflow container
        working-directory: ./pipeline/airflow
        run: |
          docker-compose exec -T airflow-scheduler pip install -r requirements.txt
     
      # Step 6: Initialize Airflow Database
      - name: Initialize Airflow Database
        working-directory: ./pipeline/airflow
        run: |
          docker-compose exec -T airflow-scheduler airflow db init

      # Step 7: Wait for Airflow Services to be Up
      - name: Wait for Airflow to initialize
        working-directory: ./pipeline/airflow
        run: |
          sleep 180  # Wait 3 minutes for Airflow to fully initialize

      # Step 8: Trigger DAG via Airflow CLI
      - name: Trigger Airflow DAG
        working-directory: ./pipeline/airflow
        run: |
          docker-compose exec -T airflow-scheduler airflow dags trigger -r manual_$(date +%Y%m%d%H%M%S) Group10_DataPipeline_MLOps
      
      # Step 9: Monitor Airflow DAG Execution 
      - name: Monitor DAG Execution
        working-directory: ./pipeline/airflow
        run: |
          while true; do
            STATUS=$(docker-compose exec -T airflow-scheduler airflow dags state Group10_DataPipeline_MLOps $(date +%Y-%m-%d))
            echo "Current DAG status: $STATUS"
            if [ "$STATUS" = "success" ]; then
              echo "DAG completed successfully"
              break
            elif [ "$STATUS" = "failed" ]; then
              echo "DAG failed"
              exit 1
            fi
            sleep 60  # Wait before checking the status again
          done

      # Step 10: Stop Airflow Services
      - name: Stop Airflow Services
        if: always()
        working-directory: ./pipeline/airflow
        run: docker-compose down --volumes --rmi all