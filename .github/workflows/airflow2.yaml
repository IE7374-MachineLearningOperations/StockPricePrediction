name: Trigger Airflow DAG Workflow

on:
  push:
    branches:
      - vy
  workflow_dispatch:

jobs:
  trigger_airflow_dag:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.10'

      # Step 4.1: Decode and Write GCP Service Account Key to File
      - name: Decode and Write GCP Service Account Key
        run: |
          echo "${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}" | base64 -d > pipeline/airflow/dags/service_key_gcs.json
        shell: bash

       # Step 4.2: Decode and Write Config to File
      - name: Decode and Write Email Config
        run: |
          echo "${{ secrets.EMAIL_AIRFLOW }}" | base64 -d > pipeline/airflow/dags/config.yaml
        shell: bash
    

      ## redundant step: check on this
      # - name: Install dependencies  
      #   run: |
      #     python -m pip install --upgrade pip
      #     pip install -r requirements.txt

      - name: Set up Docker Compose
        run: |
          sudo apt-get update
          sudo apt-get install -y docker-compose

      - name: Initialize Airflow
        working-directory: ./pipeline/airflow
        run: |
          docker-compose up airflow-init

      - name: Start Airflow Services
        working-directory: ./pipeline/airflow
        run: |
          docker-compose up -d

      # # Step 6: Install Python packages inside Docker containers
      # - name: Install Python Packages
      #   working-directory: ./pipeline/airflow
      #   run: |
      #     docker-compose exec -T airflow-scheduler python3 -m pip install -r /opt/airflow/dags/requirements.txt
      #     # docker-compose exec -T airflow-webserver python3 -m pip install -r /opt/airflow/dags/requirements.txt


      # Step 6: Set permissions for Airflow logs inside the container
      - name: Set permissions for Airflow logs
        working-directory: ./pipeline/airflow
        run: |
          docker-compose exec -T --user root airflow-scheduler bash -c "chmod -R 777 /opt/airflow/logs/"

      - name: Wait for Airflow to Initialize
        working-directory: ./pipeline/airflow
        run: |
          timeout 300 bash -c 'until docker-compose exec -T airflow-webserver curl -f http://localhost:8080/health; do sleep 10; done'
  
      # Step 9: Delete .pyc Files
      - name: Delete .pyc Files
        working-directory: ./pipeline/airflow
        run: |
          docker-compose exec -T airflow-scheduler find /opt/airflow -name \*.pyc -delete
          docker-compose exec -T airflow-webserver find /opt/airflow -name \*.pyc -delete

      - name: List DAG Import Errors
        working-directory: ./pipeline/airflow
        run: |
          docker-compose exec -T airflow-scheduler airflow dags list-import-errors

      - name: Show Airflow DAGs
        working-directory: ./pipeline/airflow
        run: |
          docker-compose exec -T airflow-scheduler airflow dags list

      - name: Trigger Airflow DAG
        working-directory: ./pipeline/airflow
        run: |
          docker-compose exec -T airflow-scheduler airflow dags trigger -r manual_$(date +%Y%m%d%H%M%S) Group10_DataPipeline_MLOps

      # - name: Monitor DAG Execution
      #   working-directory: ./pipeline/airflow
      #   run: |
      #     for i in {1..10}; do
      #       STATUS=$(docker-compose exec -T airflow-scheduler airflow dags state Group10_DataPipeline_MLOps $(date +%Y-%m-%d))
      #       echo "Current DAG status: $STATUS"
      #       if [ "$STATUS" = "success" ]; then
      #         echo "DAG completed successfully"
      #         break
      #       elif [ "$STATUS" = "failed" ]; then
      #          echo "DAG failed"
      #         exit 1
      #       fi
      #       sleep 60
      #     done
      - name: Monitor DAG Execution
        working-directory: ./pipeline/airflow
        run: |
          DAG_ID="Group10_DataPipeline_MLOps"
          EXECUTION_DATE=$(date +%Y-%m-%d)
          TASKS=$(docker-compose exec -T airflow-scheduler airflow tasks list $DAG_ID --output json | jq -r '.[]')

          for i in {1..10}; do
            ALL_SUCCESS=true
            for TASK_ID in $TASKS; do
              TASK_STATUS=$(docker-compose exec -T airflow-scheduler airflow tasks state $DAG_ID $TASK_ID $EXECUTION_DATE)
              echo "Current status of task $TASK_ID: $TASK_STATUS"
              if [ "$TASK_STATUS" != "success" ]; then
                ALL_SUCCESS=false
                if [ "$TASK_STATUS" = "failed" ]; then
                  echo "Task $TASK_ID failed"
                  exit 1
                fi
              fi
            done

            if [ "$ALL_SUCCESS" = true ]; then
              echo "All tasks completed successfully"
              break
            fi

            sleep 60
          done

      - name: Stop Airflow Services
        if: always()
        working-directory: ./pipeline/airflow
        run: docker-compose down --volumes --rmi all
      