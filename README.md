# Project Title: Stock-Price-Prediction

## Description
The primary goal of this project is to improve stock price forecasting and prediction by creating an effective Machine Learning Operations (MLOps) pipeline. Utilising sophisticated financial modelling methods like the GARCH and Kalman filters, the pipeline seeks to maximise model performance and increase the precision of stock volatility forecasts. In order to provide scalability and flexibility to new data and market trends, the project builds an automated pipeline that facilitates continuous integration and delivery of machine learning models. The system is able to react dynamically to shifting market conditions because of the smooth integration of fresh data made possible by this scalable MLOps architecture. In the end, this helps academics, data scientists, and financial analysts who want to use MLOps to make better investment decisions by producing forecasts that are more accurate.

---
## DataPipeline Assignment Phase

> Refer for [Airflow Pipeline README](https://github.com/IE7374-MachineLearningOperations/StockPricePrediction/blob/Manohar/pipeline/README.md)

>Refer [All README files](https://github.com/IE7374-MachineLearningOperations/StockPricePrediction/tree/Manohar/Assignments_Submissions/DataPipeline%20Phase) 

**Key Features:**
- Airflow DAGs for data processing pipeline, with Error Handling and Logging
- DVC for version control and data management.
- Tacking and logging, sending automated mail.
- Ability to detect and migate bias in data.
- Test Modules with pytest with no errors.
- Visualization of financial time series data.
---
## Table of Contents
1. [Project Structure](#project-structure)
2. [Environment Setup](#environment-setup)
3. [Running the Pipeline](#running-the-pipeline)
4. [Test Functions](#test-functions)
5. [Reproducibility and Data Versioning](#reproducibility-and-data-versioning)
---

## Project Structure
```bash
.
├── assets                       # Work progress and code output Visualatiizon
│   ├── correlation_matrix_after_removing_correlated_features.png
│   ├── MLOps Group10 Diagram.drawio.png
│   ├── pca_components.png
│   └── yfinance_time_series.png
├── Assignments_Submissions     # Socuments related to project phases
│   ├── DataPipeline Phase/
│   └── Scoping Phase/
├── data                        # Data storage for datasets
│   ├── ADS_Index.csv
│   ├── fama_french.csv
|      ├── FRED_Variables
│   ├── preprocessed
│   │   ├── final_dataset.csv
│   │   └── merged_original_dataset.csv
├── GCP                         # Google Cloud Platform file for data access
│   └── application_default_credentials.json
├── models                      # Directory for storing trained models (next Phase)
├── pipeline                    # Contains Airflow related DAGs, scripts, logs, and configurations
│   ├── airflow
│   │   ├── dags                # Airflow Directed Acyclic Graphs (DAG) for pipeline management
│   │   ├── logs                # Logs generated by Airflow
│   │   ├── plugins       
│   │   ├── tests/               # Pytests for pipeline components
│   │   └── working_data        
│   └── README.md               # Documentation of pipeline
├── src                         # Source code for data preprocessing, feature engineering and so
│   ├── data_preprocessing.ipynb
│   ├── DataSchema_Stats.ipynb
│   ├── Feature Engineering.ipynb
│   └── PROJECT_DATA_CLEANING.ipynb
├── tests                       # Pytests 
│   ├── test_convert_column_dtype.py
│   ├── test_correlation.py
│   └── test_scaler.py
├── README.md                   # Main project documentation
├── .gitignore             # Git ignore patterns
├── .dvcignore             # DVC ignore patterns
├── current.txt            # Repo tree structure
└── requirements.txt            # Python dependencies

```
![Project Chart](https://github.com/IE7374-MachineLearningOperations/StockPricePrediction/blob/bf7526844544398e53ca528f30e883d1d87a493c/assets/MLOps%20Group10%20Diag.png)

**Key Components:**
- `data/`: Contains raw and processed datasets 
- `pipeline/airflow/dags/`: DAGs for data processing.
- `pipeline/airflow/src/`: Core scripts for data transformation, feature engineering, and plotting.
- `tests/`: pytests for verifying code functionality.
- `assets/`: Contains generated plots for data analysis and visualization.
---

## Environment Setup

### Prerequisites

To set up and run this project, ensure the following are installed:

- **Python** (3.8 or later)
- **Docker** (for running Apache Airflow)
- **DVC** (for data version control)
- **Google Cloud SDK** (we are deploying on GCP)

### Installation Steps

1. **Clone the Repository**
   ```bash
   git clone https://github.com/IE7374-MachineLearningOperations/StockPricePrediction.git
   cd Stock-Price-Prediction
   ```

2. **Install Python Dependencies**
   Install all required packages listed in `requirements.txt`:
   ```bash
   pip install -r requirements.txt
   ```

3. **Initialize DVC**
   Set up DVC to manage large data files by pulling the tracked data:
   ```bash
   dvc pull
   ```
---

## Running the Pipeline

To execute the data pipeline, follow these steps:

1. **Start Airflow Services**
   Run Docker Compose to start services of the Airflow web server, scheduler:
   ```bash
   cd pipeline/airflow/
   docker-compose up
   ```

2. **Access Airflow UI**
   Open `http://localhost:8080` in your browser. Log into the Airflow UI and enable the DAG

3. **Trigger the DAG**
   Trigger the DAG manually to start processing. The pipeline will:
   - Ingest raw data and preprocess it.
   - Perform correlation analysis to identify redundant features.
   - Execute PCA to reduce dimensionality.
   - Generate visualizations, such as time series plots and correlation matrices.

4. **Check Outputs**
   Once completed, check the output files and images in the `assets/` folder.

or 

```sh
# Step 1: Activate virtual environment: 
cd airflow_env/ # (go to Airflow environment and open in terminal)
source bin/activate

# Step 2: Install Airflow (not required if done before)
pip install apache-airflow

# Step 3: Initialize Airflow database (not required if done before)
airflow db init

# Step 4: Start Airflow web server and airflow scheduler
airflow webserver -p 8080 & airflow scheduler

# Step 5: Access Airflow UI in your default browser
# http://localhost:8080

# Step 6: Deactivate virtual environment (after work completion)
deactivate
```

---
## Test Functions
   Run all tests in the `tests` directory
   ```bash
   pytest tests/
   ```
---
## Reproducibility and Data Versioning

We used **DVC (Data Version Control)** for files management.

### DVC Setup
1. **Initialize DVC** (not required if already initialize):
   ```bash
   dvc init
   ```

2. **Pull Data Files**
   Pull the DVC-tracked data files to ensure all required datasets are available:
   ```bash
   dvc pull
   ```

3. **Data Versioning**
   Data files are generated with `.dvc` files in the repository

4. **Tracking New Data**
   If new files are added, to track them. Example:
   ```bash
   dvc add <file-path>
   dvc push
   ```
5. **Our Project Bucket**

![GCP Bucket](https://github.com/IE7374-MachineLearningOperations/StockPricePrediction/blob/bf7526844544398e53ca528f30e883d1d87a493c/assets/gcpbucket.png)

---

## License

This project is licensed under the MIT License. See the 'LICENSE' file.
