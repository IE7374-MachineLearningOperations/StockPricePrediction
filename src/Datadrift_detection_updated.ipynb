{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#pip install google-cloud-logging"
      ],
      "metadata": {
        "id": "aBNEUKHDxMWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install datetime"
      ],
      "metadata": {
        "id": "8W_585zaxfPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/service_key.json\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import json"
      ],
      "metadata": {
        "id": "Nz7a6J0jL14r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "file_path = \"Data_pipeline_airflow_dags_data_scaled_data_train.csv\"\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "def log_feature_distribution(feature_data, feature_name, log_file):\n",
        "    \"\"\"\n",
        "    Calculate and log statistics for a feature's distribution, saving to a log file.\n",
        "    \"\"\"\n",
        "    mean = np.mean(feature_data)\n",
        "    variance = np.var(feature_data)\n",
        "    quantiles = np.percentile(feature_data, [25, 50, 75])\n",
        "\n",
        "    log_entry = {\n",
        "        \"timestamp\": str(datetime.datetime.now()),\n",
        "        \"feature\": feature_name,\n",
        "        \"mean\": mean,\n",
        "        \"variance\": variance,\n",
        "        \"25th_percentile\": quantiles[0],\n",
        "        \"50th_percentile\": quantiles[1],\n",
        "        \"75th_percentile\": quantiles[2]\n",
        "    }\n",
        "\n",
        "    with open(log_file, \"a\") as f:\n",
        "        f.write(json.dumps(log_entry, indent=4) + \"\\n\")\n",
        "\n",
        "\n",
        "def detect_drift(current_stats, baseline_stats, feature_name, log_file, threshold=0.1):\n",
        "    \"\"\"\n",
        "    Compare current statistics with baseline statistics to detect drift, saving results to a log file.\n",
        "    \"\"\"\n",
        "    drift_detected = False\n",
        "    drift_details = {}\n",
        "\n",
        "    mean_diff = abs(current_stats['mean'] - baseline_stats['mean']) / abs(baseline_stats['mean'])\n",
        "    if mean_diff > threshold:\n",
        "        drift_detected = True\n",
        "        drift_details['mean_diff'] = mean_diff\n",
        "\n",
        "    variance_diff = abs(current_stats['variance'] - baseline_stats['variance']) / abs(baseline_stats['variance'])\n",
        "    if variance_diff > threshold:\n",
        "        drift_detected = True\n",
        "        drift_details['variance_diff'] = variance_diff\n",
        "\n",
        "    for quantile in ['25th_percentile', '50th_percentile', '75th_percentile']:\n",
        "        quantile_diff = abs(current_stats[quantile] - baseline_stats[quantile]) / abs(baseline_stats[quantile])\n",
        "        if quantile_diff > threshold:\n",
        "            drift_detected = True\n",
        "            drift_details[f'{quantile}_diff'] = quantile_diff\n",
        "\n",
        "    result = {\n",
        "        \"feature\": feature_name,\n",
        "        \"drift_detected\": drift_detected,\n",
        "        \"drift_details\": drift_details\n",
        "    }\n",
        "\n",
        "    with open(log_file, \"a\") as f:\n",
        "        f.write(json.dumps(result, indent=4) + \"\\n\")\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def check_missing_update_with_grace(data, log_file, grace_days=2):\n",
        "    \"\"\"\n",
        "    Check for missing updates with a grace period before flagging data drift.\n",
        "    \"\"\"\n",
        "    latest_date = pd.to_datetime(data['date']).max()\n",
        "    today = datetime.datetime.now().date()\n",
        "\n",
        "    missing_days = (today - latest_date.date()).days\n",
        "\n",
        "    if missing_days > grace_days:\n",
        "        log_entry = {\n",
        "            \"timestamp\": str(datetime.datetime.now()),\n",
        "            \"drift_type\": \"Missing Data (Grace Period Exceeded)\",\n",
        "            \"details\": {\n",
        "                \"latest_date_in_data\": str(latest_date.date()),\n",
        "                \"expected_date\": str(today),\n",
        "                \"missing_days\": missing_days,\n",
        "                \"grace_period\": grace_days\n",
        "            }\n",
        "        }\n",
        "        with open(log_file, \"a\") as f:\n",
        "            f.write(json.dumps(log_entry, indent=4) + \"\\n\")\n",
        "        print(\"Missing data detected and logged (grace period exceeded).\")\n",
        "    else:\n",
        "        print(f\"No missing updates detected within the grace period ({grace_days} days).\")\n",
        "\n",
        "\n",
        "def run_drift_detection(data, key_features, baseline_statistics, log_file):\n",
        "    \"\"\"\n",
        "    Perform drift detection for key features in the dataset and save logs to a file.\n",
        "    \"\"\"\n",
        "    drift_results = {}\n",
        "\n",
        "    for feature in key_features:\n",
        "        feature_data = data[feature]\n",
        "        log_feature_distribution(feature_data, feature, log_file)\n",
        "\n",
        "        baseline_stats = {\n",
        "            \"mean\": baseline_statistics.loc['mean', feature],\n",
        "            \"variance\": baseline_statistics.loc['std', feature] ** 2,\n",
        "            \"25th_percentile\": baseline_statistics.loc['25%', feature],\n",
        "            \"50th_percentile\": baseline_statistics.loc['50%', feature],\n",
        "            \"75th_percentile\": baseline_statistics.loc['75%', feature],\n",
        "        }\n",
        "\n",
        "        current_stats = {\n",
        "            \"mean\": feature_data.mean(),\n",
        "            \"variance\": feature_data.var(),\n",
        "            \"25th_percentile\": feature_data.quantile(0.25),\n",
        "            \"50th_percentile\": feature_data.median(),\n",
        "            \"75th_percentile\": feature_data.quantile(0.75),\n",
        "        }\n",
        "\n",
        "        drift_results[feature] = detect_drift(current_stats, baseline_stats, feature, log_file)\n",
        "\n",
        "    return drift_results\n",
        "\n",
        "\n",
        "# Define key features and log file\n",
        "key_features = ['volume', 'RSI', 'MACD', 'MA20', 'SP500_VIXCLS_ratio']\n",
        "baseline_statistics = data[key_features].describe(percentiles=[0.25, 0.5, 0.75])\n",
        "log_file = \"drift_detection_log.txt\"\n",
        "\n",
        "# Initialize log file\n",
        "with open(log_file, \"w\") as f:\n",
        "    f.write(\"Drift Detection Insights:\\n\")\n",
        "\n",
        "# Check for missing updates\n",
        "check_missing_update_with_grace(data, log_file, grace_days=2)\n",
        "\n",
        "# Run drift detection if updates are recent\n",
        "if pd.to_datetime(data['date']).max().date() == datetime.datetime.now().date():\n",
        "    drift_results = run_drift_detection(data.iloc[-10:], key_features, baseline_statistics, log_file)\n",
        "else:\n",
        "    print(\"Skipping drift detection due to missing updates.\")\n"
      ],
      "metadata": {
        "id": "j9f0nbnb0lHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import logging as cloud_logging\n",
        "\n",
        "# Initialize Cloud Logging client\n",
        "cloud_logging_client = cloud_logging.Client()\n",
        "logger = cloud_logging_client.logger(\"drift-detection-logs\")  # Log name\n",
        "\n",
        "def log_to_cloud_logging(message):\n",
        "    \"\"\"\n",
        "    Log a message to Google Cloud Logging.\n",
        "    \"\"\"\n",
        "    logger.log_text(message)"
      ],
      "metadata": {
        "id": "2INMPZcKKzFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_to_cloud_logging(\"Drift detection setup test successful.\")\n"
      ],
      "metadata": {
        "id": "1zmxfhsnK2Xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_to_cloud_logging(json.dumps(drift_results, indent=4))\n"
      ],
      "metadata": {
        "id": "fSPOQN_TLTDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VWUt2RkELVIP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}